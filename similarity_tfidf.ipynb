{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"tfidf.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "def multiprocess_tasks(func, tasks, data, njobs=cpu_count()):\n",
    "    wrapper = partial(func, df=data)\n",
    "    res = []\n",
    "    with Pool(processes=njobs) as p:\n",
    "        n = len(tasks)\n",
    "        with tqdm(total=n) as pbar:\n",
    "            for i, v in enumerate(p.imap_unordered(wrapper, tasks)):\n",
    "                res.append(v)\n",
    "                pbar.update()\n",
    "    return res\n",
    "\n",
    "\n",
    "def r_squared_adj(data, group, dummy=True, var_name=\"palt\"):\n",
    "    y = data[var_name]\n",
    "    if dummy:\n",
    "        x = data[group]\n",
    "    else:\n",
    "        x = pd.get_dummies(data[group], drop_first=True)\n",
    "    x = sm.add_constant(x)\n",
    "    mod = sm.OLS(y, x).fit()\n",
    "    return mod.rsquared_adj.round(2)\n",
    "\n",
    "\n",
    "def match_token(text, token):\n",
    "    tokens = tokenize_only(text)\n",
    "    if token in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def token_power(token, df, desc_column=\"award_description\"):\n",
    "    df[token] = df[desc_column].apply(match_token, token=token)\n",
    "    r2 = r_squared_adj(df, token)\n",
    "    return {token: r2}\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [\n",
    "        word.lower()\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        for word in nltk.word_tokenize(sent)\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process award description of IRS contracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 284)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# read in all other contract\n",
    "dfs = []\n",
    "for fp in glob.glob('data/treasury/*.zip'):\n",
    "    if '2019' in fp or '2020' in fp:\n",
    "        df = pd.read_csv(fp, low_memory=False, dtype={'modification_number':str, 'parent_award_agency_id':str, 'awarding_sub_agency_code':str, 'funding_sub_agency_code':str})\n",
    "        df['fy'] = os.path.basename(fp)[2:6]\n",
    "        dfs.append(df)\n",
    "dat = pd.concat(dfs)\n",
    "\n",
    "# IRS contracts only\n",
    "dat = dat.loc[(dat.modification_number=='0')&((dat.parent_award_agency_id=='2050')|(dat.awarding_sub_agency_code=='2050')|(dat.funding_sub_agency_code=='2050'))]\n",
    "\n",
    "dat['action_date'] = pd.to_datetime(dat['action_date'])\n",
    "dat['solicitation_date'] = pd.to_datetime(dat['solicitation_date'])\n",
    "dat['palt'] = (dat['action_date'] - dat['solicitation_date']).dt.days\n",
    "dat = dat.loc[dat.palt.notnull()].reset_index(drop=True)\n",
    "\n",
    "# # select 95%tile for outlier cutoff\n",
    "palt = dat.loc[dat.palt<322].reset_index(drop=True)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process award description of Splunk contracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 135)\n",
      "['annual', 'brand', 'brand name', 'enterpris', 'enterpris licens', 'hardwar', 'licens', 'mainten', 'name', 'order', 'profession', 'purchas', 'purpos', 'renew', 'requir', 'secur', 'server', 'softwar', 'softwar licens', 'splunk enterpris', 'splunk enterpris licens', 'splunk licens', 'splunk profession', 'splunk softwar', 'splunk softwar licens', 'standard', 'term']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exerci'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# read in splunk\n",
    "splunk = pd.read_csv('data/USA3_Splunk_slim.csv')\n",
    "print(splunk.shape)\n",
    "\n",
    "max_df=0.8\n",
    "max_features=2000\n",
    "min_df=0.04\n",
    "use_idf=True\n",
    "tokenizer=tokenize_and_stem\n",
    "ngram_range=(1, 3)\n",
    "random_state=1111\n",
    "\n",
    "domain_stopwords = [\"OT\", \"CL\", \"CT\", \"option\", \"closeout\", \"contractor\", \"contract\", \"fund\", \"funding\", \"funds\", \"exercise\", \"service\", \"services\", \"cyrbyme\", \"support\", \"igf\"]\n",
    "\n",
    "en_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords = en_stopwords + [i.lower() for i in domain_stopwords]\n",
    "stopwords = tokenize_and_stem(\" \".join(stopwords))\n",
    "\n",
    "logging.info(\n",
    "    f\"max_df={max_df}, max_features={max_features}, min_df={min_df}, use_idf={use_idf}, ngram_range={ngram_range}, domain_stopwords={domain_stopwords}\"\n",
    ")\n",
    "\n",
    "# define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=max_df,\n",
    "    max_features=max_features,\n",
    "    min_df=min_df,\n",
    "    stop_words=stopwords,\n",
    "    use_idf=use_idf,\n",
    "    tokenizer=tokenizer,\n",
    "    ngram_range=ngram_range,\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(splunk[\"award_description\"].tolist())\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "logging.info(f\"{len(terms)} features extracted: {terms}\")\n",
    "\n",
    "splunk_terms = tfidf_matrix.toarray()\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure and rank order similarity between all IRS contracts and Splunk contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>award_description</th>\n",
       "      <th>Cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>PROSCI ENTERPRISE CHANGE MANAGEMENT SITE LICEN...</td>\n",
       "      <td>0.593653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>BLACKBERRY ENTERPRISE MOBILITY LICENSE PROGRAM</td>\n",
       "      <td>0.593653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>VERITAS INFOSCALE SOFTWARE SUITE LICENSES AND ...</td>\n",
       "      <td>0.543943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>IRS ORACLE ENTERPRISE LICENSE AGREEMENT (ELA) ...</td>\n",
       "      <td>0.543002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>ENTERPRISE PHYSICAL ACCESS CONTROL SYSTEM (EPA...</td>\n",
       "      <td>0.541499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     award_description  Cosine similarity\n",
       "788  PROSCI ENTERPRISE CHANGE MANAGEMENT SITE LICEN...           0.593653\n",
       "605     BLACKBERRY ENTERPRISE MOBILITY LICENSE PROGRAM           0.593653\n",
       "669  VERITAS INFOSCALE SOFTWARE SUITE LICENSES AND ...           0.543943\n",
       "411  IRS ORACLE ENTERPRISE LICENSE AGREEMENT (ELA) ...           0.543002\n",
       "410  ENTERPRISE PHYSICAL ACCESS CONTROL SYSTEM (EPA...           0.541499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splunk center\n",
    "np.random.seed(random_state)\n",
    "km = KMeans(n_clusters=1)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "splunk_center = km.cluster_centers_\n",
    "\n",
    "# project all contract award_description onto tf-idf feature space\n",
    "dat_vec = tfidf_vectorizer.transform(dat[\"award_description\"].tolist()).toarray()\n",
    "dat[terms] = pd.DataFrame(dat_vec)\n",
    "\n",
    "# calculate cosine similarity to the splunk center\n",
    "cs = cosine_similarity(splunk_center, dat_vec)\n",
    "dat[\"Cosine similarity\"] = cs.squeeze()\n",
    "dat = dat.sort_values(by=\"Cosine similarity\", ascending=False)\n",
    "\n",
    "dat[[\"award_description\", \"Cosine similarity\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "dat[[\"award_id_piid\",\"award_description\"] + terms + [\"Cosine similarity\"]].to_csv('output/splunk_similarity_tfidf.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
